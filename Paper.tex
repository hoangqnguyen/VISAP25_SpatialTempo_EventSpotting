% TODO:
%   [ ] Write Method
%   [ ] Write Data:
    % Not all of images are annotated -> We will add more annotations as time goes by
    % # Frames:  890797
            %     ********************
            % Split: all
            % # Clips (Rallies):  947
            % # Events:  5935
            % # Frames:  257388
            % ********************
            % Split: train
            % # Clips (Rallies):  561
            % # Events:  3631
            % # Frames:  156818
            % ********************
            % Split: val
            % # Clips (Rallies):  220
            % # Events:  1356
            % # Frames:  58430
            % ********************
            % Split: test
            % # Clips (Rallies):  166
            % # Events:  948
            % # Frames:  42140

%   [ ] Write Experiments
%   [ ] Create Diagrams
    %   [ ] Model Architecture



\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
\usepackage{subcaption}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{algorithm2e}
\usepackage[bottom]{footmisc}
\usepackage{SCITEPRESS}     % Please add other packages that you may need BEFORE the SCITEPRESS.sty package.
\usepackage{hyperref}

\begin{document}

% \title{Multi-task Architecture for Temporally and Spatially Precise Event Spotting in Volleyball Videos}
% \title{Dual-Objective Deep Learning Architecture for Accurate Temporal and Spatial Event Detection in Volleyball}
\title{Multi-Task Temporal and Spatial Networks for High-Precision Event Spotting in Volleyball Videos}
% \title{Multi-Task Temporal-Spatial Deep Learning for Accurate Event Localization in Volleyball Videos}
% \title{Multi-Task Deep Framework for Accurate Temporal and Spatial Event Detection in Volleyball Videos}

\author{\authorname{Hoang Quoc Nguyen\sup{1}\sup{2}\orcidAuthor{0009-0002-2004-9285}, Second Author Name\sup{1}\orcidAuthor{0000-0000-0000-0000} and Third Author Name\sup{2}\orcidAuthor{0000-0000-0000-0000}}
\affiliation{\sup{1}Korea Institute of Science and Technology, Seoul, Republic of Korea}
\affiliation{\sup{2}University of Science and Technology, Daejeon, Republic of Korea}
\email{523503, second\_author\}@kist.re.kr, third\_author@dc.mu.edu}
}

\keywords{Precise Event Spotting, Video Understanding, Spatial Temporal Event Spotting, Deep Learning, Volleyball, Sport}

\abstract{
  Understanding the precise timing and location of events is crucial for analyzing sports videos, especially in fast-paced sports like volleyball. We introduce a new task: high-precision spatial-temporal event spotting, which aims to detect both when and where key actions occur. To support this, we present the KOVO Volleyball Event Dataset, featuring 947 rally videos, and 5,935 events, annotated for both temporal and spatial localization. Our best model achieves a combined mAP of 85.46 across various temporal and spatial thresholds. Notably, we find that incorporating spatial predictions enhances temporal mAP by 5.89 points, underscoring the synergy between spatial and temporal analysis. To the best of our knowledge, this is the first work addressing this task, establishing a strong baseline for future research in spatial-temporal event spotting.
}



\onecolumn \maketitle \normalsize \setcounter{footnote}{0} \vfill

\section{\uppercase{Introduction}}
\label{sec:introduction}
Video understanding has emerged as a cornerstone in computer vision, offering valuable insights into dynamic scenes for applications such as sports analytics, surveillance, and autonomous systems. This field encompasses various tasks designed to interpret and analyze actions over time. Among these, \textit{Video Classification} aims to assign a single label to an entire video, providing a broad understanding of the content but often lacking frame-level precision. In contrast, \textit{Temporal Action Localization (TAL)} focuses on identifying time intervals where specific actions occur within untrimmed videos. Complementing these is \textit{Precise Action Spotting (PES)}, which identifies the precise frames that capture key events, requiring models to discern subtle temporal differences and distinguish visually similar frames \cite{spot22}.

Recent advancements in action spotting, such as \textit{T-DEED} \cite{tdeed23} and \textit{E2E-Spot} \cite{spot22}, have demonstrated the ability of models to achieve frame-level precision in fast-paced events using deep learning architectures. Datasets like \textit{FigureSkating} \cite{figureskating} and \textit{FineDiving} \cite{finediving} have been pivotal in advancing action spotting, emphasizing the importance of precise temporal detection in sports with individual athletes. However, these datasets are tailored to specific sports and do not capture the complexity and rapid dynamics of team-based, high-speed sports, such as volleyball.

In volleyball, rapid play transitions occur within specific areas of the court, making precise spatial localization as important as temporal accuracy. To address this, we introduce the new task of \textit{high-precision spatial-temporal event spotting}, designed to detect both the exact timing and spatial location of key events. Unlike conventional action spotting, this task provides richer insights into player positioning and movement patterns, which are crucial for analyzing volleyball gameplay.

In other sports, datasets like \textit{SoccerNet-v2} \cite{soccernetv2} have pushed the boundaries of action spotting through rich temporal and spatial annotations, significantly advancing model capabilities. Yet, no equivalent dataset exists for volleyball, a sport characterized by its rapid exchanges and the need for precise localization of actions. To fill this gap, we introduce the \textit{KOVO Event Dataset}, comprising 947 rally videos, 890,797 frames, and 5,935 annotated key actions. This dataset offers granular annotations for both temporal and spatial event localization, making it a valuable resource for developing models that capture the intricacies of volleyball.

Our contributions are threefold:
\begin{itemize}
    \item \textbf{New Task Introduction:} We introduce the task of high-precision spatial-temporal event spotting, specifically tailored for the dynamics of volleyball.
    \item \textbf{Dataset Development:} We present the \textit{KOVO Event Dataset}, the first of its kind to include detailed temporal and spatial annotations for volleyball rallies, aimed at fostering research in this area.
    \item \textbf{Model Development:} We propose a multi-task deep learning model that jointly predicts event timing and spatial positions, leveraging this dual focus to achieve improved performance. Notably, incorporating spatial predictions into our model enhances temporal mAP by 5.89 points.
\end{itemize}

Our best model achieves a temporal mAP of 90.59, a spatial mAP of 77.94, and a combined mAP of 85.46, providing a strong baseline for this new task. To the best of our knowledge, this work is the first to explore high-precision spatial-temporal event spotting in volleyball, setting the stage for future research in this area.


% TODO: Fill this when finish paper
% The paper proceeds with a discussion of related work in Section 2, a detailed description of our approach in Section 3, the experimental setup in Section 4, results in Section 5, and conclusions in Section 6.



\section{\uppercase{Related work}}

\subsection{Video Classification}
Video classification aims to predict a single label for an entire video, in contrast to event spotting, which requires precise frame-level labeling. This distinction introduces unique challenges: video classification can leverage sparse frame sampling \cite{tsn}, whereas event spotting demands dense sampling to capture rapid changes in events. Additionally, classification models often employ global space-time pooling \cite{8578773} or temporal consensus \cite{zhou2018temporalrelationalreasoningvideos} to produce video-level predictions, while event spotting necessitates preserving high temporal resolution.


\subsection{Temporal Action Localization}
Temporal Action Localization (TAL) aims to identify the time intervals when specific actions occur in untrimmed videos, making it ideal for longer actions that are not instantaneous. Unlike video classification, which assigns a single label to an entire video, TAL requires precise start and end times, making it more complex.

TAL methods are typically categorized into two groups: two-stage \cite{9578330,10.1007/978-3-319-46487-9_47} and one-stage \cite{10203543,zhang2022actionformer} approaches. Two-stage models generate action proposals before classifying them, while one-stage models directly predict actions and their intervals in a streamlined process. Recent methods, like ActionFormer \cite{zhang2022actionformer} and TriDet \cite{tridet}, leverage advanced architectures, including transformers and feature pyramids, to improve temporal precision across varying action durations. Anchor-free approaches \cite{9171561} have further enhanced flexibility in predicting actions without relying on predefined time windows.

TAL's development has been driven by extensive datasets and benchmarks such as ActivityNet \cite{ActivityNet}, EPIC-KITCHENS \cite{damen2018scalingegocentricvisionepickitchens}, and THUMOS Challenge \cite{Idrees_2017}, making it a well-explored field for understanding complex, prolonged actions in videos. However, it remains distinct from action spotting, which focuses on identifying brief, precise moments in fast-paced scenarios.
\subsection{Precise Event Spotting (PES)}

Precise Event Spotting (PES) aims to detect the exact frames of key events in untrimmed videos, making it ideal for fast, critical moments in sports. Unlike Temporal Action Localization (TAL), which spans broader time intervals, PES requires frame-level accuracy, crucial for detailed sports analysis where slight timing shifts can alter game interpretations.

Datasets like \textit{FigureSkating} \cite{figureskating} and \textit{FineDiving} \cite{finediving} have advanced PES with frame-level annotations, but focus on simpler, individual sports. Recent methods, such as \textit{T-DEED} \cite{tdeed23} and \textit{E2E-Spot} \cite{spot22}, enhance precision by refining temporal representations and avoiding pooling, but lack spatial context needed for team sports.

In volleyball, spatial localization is as critical as timing. Understanding event locations is key to analyzing player movements and strategies. Our work addresses this by introducing a new task and dataset for high-precision spatial-temporal event spotting in volleyball, capturing both event timing and location.

Inspired by \textit{E2E-Spot} \cite{spot22}, our approach uses RegNet-Y \cite{radosavovic2020designingnetworkdesignspaces} with GSM \cite{9156729} for adaptive temporal shifts. This combination balances efficiency and precision, making it ideal for capturing complex spatial-temporal dynamics in volleyball.



\section{\uppercase{DATASET OVERVIEW}}
\subsection{Data Content and Statistics}
\subsection{Annotation Process}
\subsection{Dataset Splits}
\subsection{Release and Access}
Due to the large size of the dataset, we are unable to release the full-resolution (1280x720) videos in this paper. However, we have made a resized version (512x288 resolution) along with the corresponding annotations available on Kaggle, totaling 100GB. The dataset can be accessed at \url{www.kaggle.com/soon}.


\section{\uppercase{PROPOSED METHOD}}
\subsection{Problem Formulation}
\textbf{Spatial-Temporal Event Spotting (STES)} aims to identify both the precise time and location of events within untrimmed videos. Given a video with \(N\) frames \(x_1, \ldots, x_N\) and a set of \(K\) event classes \(c_1, \ldots, c_K\), the goal is to predict a sparse set of frame indices where events occur, as well as the corresponding event class and spatial coordinates. Each prediction is represented as \((t, c_t, s_t)\), where \(t\) is the frame index, \(c_t\) is the predicted event class, and \(s_t\) is the spatial location of the event within that frame. A temporal prediction is considered correct if it falls within a small temporal tolerance \(\sigma_f\) frames of the labeled event and matches the ground-truth class. Similarly, spatial predictions are deemed correct if the distance between the predicted location \(s_t\) and the ground-truth event location is within a specified threshold \(\sigma_p\). For STES, these tolerances are kept small, requiring precise frame-level accuracy, and it assumes that videos are captured with sufficiently high frame rates.

To perform well on this task, several key requirements must be met:
\begin{itemize}
    \item \textbf{Local Spatial-Temporal Features}: The model should capture subtle visual changes and movements across neighboring frames, essential for distinguishing between similar-looking moments.
    \item \textbf{Long-Term Temporal Context}: A broader temporal window allows the model to understand events in context, such as how player positions or actions evolve before and after a critical moment.
    \item \textbf{Dense Per-Frame Predictions}: For each frame \(x_t\), the model produces a prediction \((\hat{y}_t, \hat{s}_t)\), where \(\hat{y}_t \in \mathbb{R}^K\) is a vector of logits representing class probabilities, and \(\hat{s}_t \in \mathbb{R}^2\) represents the \(x, y\) coordinates of the event within the frame. The final class \(c_t\) is obtained by applying argmax to \(\hat{y}_t\), ensuring frame-level precision for each event.
\end{itemize}

These requirements call for a robust and end-to-end trainable architecture capable of leveraging both temporal and spatial information. Our approach utilizes a sequence model that integrates local spatial-temporal features.





\subsection{Model Architecture}
\subsubsection{Feature Extractor}
\subsubsection{Temporal Event Detection}
\subsubsection{Spatial Event Detection}
\subsubsection{Multi-Task Learning}
\subsubsection{Loss Function}


\section{\uppercase{EXPERIMENTS}}
\label{sec:experiments}
\subsection{Implementation Details}
\subsection{Training Strategy}
\subsection{Evaluation Metrics}

\section{\uppercase{CONCLUSIONS}}
\label{sec:conclusions}

Please note that ONLY the files required to compile your paper should be submitted. Previous versions or examples MUST be removed from the compilation directory before submission.

We hope you find the information in this template useful in the preparation of your submission.

\section*{\uppercase{Acknowledgements}}

If any, should be placed before the references section
without numbering. To do so please use the following command:

% \textit{$\backslash$section*\{ACKNOWLEDGEMENTS\}}


\bibliographystyle{apalike}
{\small
\bibliography{cite}}


\section*{\uppercase{Appendix}}

If any, the appendix should appear directly after the
references without numbering, and not on a new page. To do so please use the following command:
% \textit{$\backslash$section*\{APPENDIX\}}

\end{document} 

