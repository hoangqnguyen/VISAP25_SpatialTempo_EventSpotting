\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
\usepackage{subcaption}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{algorithm2e}
\usepackage[bottom]{footmisc}
\usepackage{SCITEPRESS}     % Please add other packages that you may need BEFORE the SCITEPRESS.sty package.

\begin{document}

% \title{Multi-task Architecture for Temporally and Spatially Precise Event Spotting in Volleyball Videos}
% \title{Dual-Objective Deep Learning Architecture for Accurate Temporal and Spatial Event Detection in Volleyball}
\title{Multi-Task Temporal and Spatial Networks for High-Precision Event Spotting in Volleyball Videos}
% \title{Multi-Task Temporal-Spatial Deep Learning for Accurate Event Localization in Volleyball Videos}
% \title{Multi-Task Deep Framework for Accurate Temporal and Spatial Event Detection in Volleyball Videos}

\author{\authorname{Hoang Quoc Nguyen\sup{1}\sup{2}\orcidAuthor{0009-0002-2004-9285}, Second Author Name\sup{1}\orcidAuthor{0000-0000-0000-0000} and Third Author Name\sup{2}\orcidAuthor{0000-0000-0000-0000}}
\affiliation{\sup{1}Korea Institute of Science and Technology, Seoul, Republic of Korea}
\affiliation{\sup{2}University of Science and Technology, Daejeon, Republic of Korea}
\email{523503, second\_author\}@kist.re.kr, third\_author@dc.mu.edu}
}

\keywords{Temporally Precise Spotting, Video Understanding, Spatial Temporal Event Spotting, Volleyball, Sport, Deep Learning}

\abstract{
  Understanding the precise timing and location of events is crucial for analyzing sports videos, especially in fast-paced sports like volleyball. We introduce a new task: high-precision spatial-temporal event spotting, which aims to detect both when and where key actions occur. To support this, we present the KOVO Volleyball Event Dataset, featuring 947 rally videos, and 5,935 events, annotated for both temporal and spatial localization. Our best model achieves a combined mAP of 85.46 across various temporal and spatial thresholds. Notably, we find that incorporating spatial predictions enhances temporal mAP by 5.89 points, underscoring the synergy between spatial and temporal analysis. To the best of our knowledge, this is the first work addressing this task, establishing a strong baseline for future research in spatial-temporal event spotting.
}



\onecolumn \maketitle \normalsize \setcounter{footnote}{0} \vfill

\section{\uppercase{Introduction}}
\label{sec:introduction}
Video understanding has emerged as a cornerstone in computer vision, offering valuable insights into dynamic scenes for applications such as sports analytics, surveillance, and autonomous systems. Within this field, various tasks have been defined to interpret actions over time. \textit{Temporal Action Detection (TAD)} focuses on pinpointing time intervals where specific actions occur within untrimmed videos, while \textit{Temporal Action Segmentation (TAS)} aims to divide videos into continuous sequences of actions. Complementing these is the task of \textit{Action Spotting}, which identifies the precise frames that capture key events, requiring models to discern subtle temporal differences and visually similar frames \cite{spot22}.

Recent advancements in action spotting, such as \textit{T-DEED} \cite{tdeed23} and \textit{E2E-Spot} \cite{spot22}, have demonstrated the ability of models to achieve frame-level precision in fast-paced events using deep learning architectures. Datasets like \textit{FigureSkating} \cite{figureskating} and \textit{FineDiving} \cite{finediving} have been pivotal in advancing action spotting, emphasizing the importance of precise temporal detection in sports with individual athletes. However, these datasets are tailored to specific sports and do not capture the complexity and rapid dynamics of team-based, high-speed sports, such as volleyball.

In volleyball, rapid play transitions occur within specific areas of the court, making precise spatial localization as important as temporal accuracy. To address this, we introduce the new task of \textit{high-precision spatial-temporal event spotting}, designed to detect both the exact timing and spatial location of key events. Unlike conventional action spotting, this task provides richer insights into player positioning and movement patterns, crucial for analyzing volleyball gameplay.

In other sports, datasets like \textit{SoccerNet-v2} \cite{soccernetv2} have pushed the boundaries of action spotting through rich temporal and spatial annotations, significantly advancing model capabilities. Yet, no equivalent dataset exists for volleyball, a sport characterized by its rapid exchanges and the need for precise localization of actions. To fill this gap, we introduce the \textit{KOVO Event Dataset}, comprising 947 rally videos, 890,797 frames, and 5,935 annotated key actions. This dataset offers granular annotations for both temporal and spatial event localization, making it a valuable resource for developing models that capture the intricacies of volleyball.

Our contributions are threefold. First, we introduce the new task of high-precision spatial-temporal event spotting, specifically tailored for the dynamics of volleyball. Second, we present the \textit{KOVO Event Dataset}, the first of its kind to include detailed temporal and spatial annotations for volleyball rallies. Third, we propose a multi-task deep learning model that jointly predicts event timing and spatial positions, leveraging this dual focus to achieve improved performance. Notably, incorporating spatial predictions into our model enhances temporal mAP by 5.89 points. Our best model achieves a temporal mAP of 90.59, a spatial mAP of 77.94, and a combined mAP of 85.46, providing a strong baseline for this new task. To the best of our knowledge, this work is the first to explore high-precision spatial-temporal event spotting in volleyball, setting the stage for future research in this area.

% TODO: Fill this when finish paper
% The paper proceeds with a discussion of related work in Section 2, a detailed description of our approach in Section 3, the experimental setup in Section 4, results in Section 5, and conclusions in Section 6.



\section{\uppercase{Related work}}

\subsection{Video Classification}
In video understanding, video classification focuses on predicting a single label for the entire video, unlike event spotting, which demands precise frame-level labeling. This difference leads to distinct challenges: video classification often benefits from sparse frame sampling \cite{tsn}, while event spotting requires dense sampling to capture rapid changes in events. Additionally, classification models frequently use techniques like global space-time pooling \cite{8578773} or temporal consensus \cite{zhou2018temporalrelationalreasoningvideos} to derive a video-level prediction, which contrasts with the need for maintaining high temporal resolution in event spotting.

Drawing from insights provided by E2E-Spot, which demonstrated the advantages of end-to-end training without temporal pooling for frame-level precision, our approach adopts RegNet-Y \cite{radosavovic2020designingnetworkdesignspaces} combined with GSM \cite{9156729}. RegNet-Y, known for its efficient architecture, paired with GSM for adaptive temporal shifts, offers a robust solution for extracting spatial-temporal features. This combination proved particularly effective for our high-precision event spotting task in volleyball, enabling both temporal accuracy and spatial precision while keeping the process efficient.

\subsection{Group Activity Recognition}


\subsection{Precise Action Spotting}


\section{\uppercase{DATASET OVERVIEW}}
\subsection{Data Content and Statistics}
\subsection{Annotation Process}
\subsection{Dataset Splits}

\section{\uppercase{PROPOSED METHOD}}
\subsection{Problem Formulation}
\subsection{Model Architecture}
\subsubsection{Feature Extractor}
\subsubsection{Temporal Event Detection}
\subsubsection{Spatial Event Detection}
\subsubsection{Multi-Task Learning}
\subsubsection{Loss Function}


\section{\uppercase{EXPERIMENTS}}
\label{sec:experiments}
\subsection{Implementation Details}
\subsection{Training Strategy}
\subsection{Evaluation Metrics}

\section{\uppercase{CONCLUSIONS}}
\label{sec:conclusions}

Please note that ONLY the files required to compile your paper should be submitted. Previous versions or examples MUST be removed from the compilation directory before submission.

We hope you find the information in this template useful in the preparation of your submission.

\section*{\uppercase{Acknowledgements}}

If any, should be placed before the references section
without numbering. To do so please use the following command:

% \textit{$\backslash$section*\{ACKNOWLEDGEMENTS\}}


\bibliographystyle{apalike}
{\small
\bibliography{cite}}


\section*{\uppercase{Appendix}}

If any, the appendix should appear directly after the
references without numbering, and not on a new page. To do so please use the following command:
% \textit{$\backslash$section*\{APPENDIX\}}

\end{document} 

