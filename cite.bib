@misc{tdeed23,
  title         = {T-DEED: Temporal-Discriminability Enhancer Encoder-Decoder for Precise Event Spotting in Sports Videos},
  author        = {Artur Xarles and Sergio Escalera and Thomas B. Moeslund and Albert Clapés},
  year          = {2024},
  eprint        = {2404.05392},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2404.05392}
}
@misc{spot22,
  title         = {Spotting Temporally Precise, Fine-Grained Events in Video},
  author        = {James Hong and Haotian Zhang and Michaël Gharbi and Matthew Fisher and Kayvon Fatahalian},
  year          = {2022},
  eprint        = {2207.10213},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2207.10213}
}

@inproceedings{soccernetv2,
  title     = {Soccernet-v2: A dataset and benchmarks for holistic understanding of broadcast soccer videos},
  author    = {Deliege, Adrien and Cioppa, Anthony and Giancola, Silvio and Seikavandi, Meisam J and Dueholm, Jacob V and Nasrollahi, Kamal and Ghanem, Bernard and Moeslund, Thomas B and Van Droogenbroeck, Marc},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages     = {4508--4519},
  year      = {2021}
}

@inproceedings{soccernet,
  title     = {SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos},
  url       = {http://dx.doi.org/10.1109/CVPRW.2018.00223},
  doi       = {10.1109/cvprw.2018.00223},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  publisher = {IEEE},
  author    = {Giancola, Silvio and Amine, Mohieddine and Dghaily, Tarek and Ghanem, Bernard},
  year      = {2018},
  month     = jun
}


@misc{finediving,
  title         = {FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment},
  author        = {Jinglin Xu and Yongming Rao and Xumin Yu and Guangyi Chen and Jie Zhou and Jiwen Lu},
  year          = {2022},
  eprint        = {2204.03646},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2204.03646}
}

@inproceedings{figureskating,
  author    = {Hong, James and Fisher, Matthew and Gharbi, Michaël and Fatahalian, Kayvon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Video Pose Distillation for Few-Shot, Fine-Grained Sports Action Recognition},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {9234-9243},
  keywords  = {Visualization;Target recognition;Impedance matching;Pose estimation;Detectors;Feature extraction;Pattern recognition;Transfer/Low-shot/Semi/Unsupervised Learning;Action and behavior recognition;Gestures and body pose;Representation learning;Video analysis and understanding},
  doi       = {10.1109/ICCV48922.2021.00912}
}

@inproceedings{tsn,
  author    = {Wang, Limin
               and Xiong, Yuanjun
               and Wang, Zhe
               and Qiao, Yu
               and Lin, Dahua
               and Tang, Xiaoou
               and Van Gool, Luc},
  editor    = {Leibe, Bastian
               and Matas, Jiri
               and Sebe, Nicu
               and Welling, Max},
  title     = {Temporal Segment Networks: Towards Good Practices for Deep Action Recognition},
  booktitle = {Computer Vision -- ECCV 2016},
  year      = {2016},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {20--36},
  abstract  = {Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ({\$}{\$} 69.4{\backslash},{\backslash}{\%} {\$}{\$}) and UCF101 ({\$}{\$} 94.2{\backslash},{\backslash}{\%} {\$}{\$}). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices (Models and code at https://github.com/yjxiong/temporal-segment-networks).},
  isbn      = {978-3-319-46484-8}
}

@INPROCEEDINGS{8578773,
  author={Tran, Du and Wang, Heng and Torresani, Lorenzo and Ray, Jamie and LeCun, Yann and Paluri, Manohar},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={A Closer Look at Spatiotemporal Convolutions for Action Recognition}, 
  year={2018},
  volume={},
  number={},
  pages={6450-6459},
  keywords={Three-dimensional displays;Two dimensional displays;Spatiotemporal phenomena;Solid modeling;Feature extraction;Computer architecture},
  doi={10.1109/CVPR.2018.00675}}

@misc{zhou2018temporalrelationalreasoningvideos,
      title={Temporal Relational Reasoning in Videos}, 
      author={Bolei Zhou and Alex Andonian and Aude Oliva and Antonio Torralba},
      year={2018},
      eprint={1711.08496},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1711.08496}, 
}

@misc{radosavovic2020designingnetworkdesignspaces,
      title={Designing Network Design Spaces}, 
      author={Ilija Radosavovic and Raj Prateek Kosaraju and Ross Girshick and Kaiming He and Piotr Dollár},
      year={2020},
      eprint={2003.13678},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2003.13678}, 
}


@INPROCEEDINGS{9156729,
  author={Sudhakaran, Swathikiran and Escalera, Sergio and Lanz, Oswald},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Gate-Shift Networks for Video Action Recognition}, 
  year={2020},
  volume={},
  number={},
  pages={1099-1108},
  keywords={Two dimensional displays;Three-dimensional displays;GSM;Convolution;Feature extraction;Logic gates;Kernel},
  doi={10.1109/CVPR42600.2020.00118}}


@INPROCEEDINGS {9578330,
author = {Z. Qing and H. Su and W. Gan and D. Wang and W. Wu and X. Wang and Y. Qiao and J. Yan and C. Gao and N. Sang},
booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Temporal Context Aggregation Network for Temporal Action Proposal Refinement},
year = {2021},
volume = {},
issn = {},
pages = {485-494},
abstract = {Temporal action proposal generation aims to estimate temporal intervals of actions in untrimmed videos, which is a challenging yet important task in the video understanding field. The proposals generated by current methods still suffer from inaccurate temporal boundaries and inferior confidence used for retrieval owing to the lack of efficient temporal modeling and effective boundary context utilization. In this paper, we propose Temporal Context Aggregation Network (TCANet) to generate high-quality action proposals through &quot;local and global&quot; temporal context aggregation and complementary as well as progressive boundary refinement. Specifically, we first design a Local-Global Temporal Encoder (LGTE), which adopts the channel grouping strategy to efficiently encode both &quot;local and global&quot; temporal inter-dependencies. Furthermore, both the boundary and internal context of proposals are adopted for frame-level and segment-level boundary regressions, respectively. Temporal Boundary Regressor (TBR) is designed to combine these two regression granularities in an end-to-end fashion, which achieves the precise boundaries and reliable confidence of proposals through progressive refinement. Extensive experiments are conducted on three challenging datasets: HACS, ActivityNet-v1.3, and THUMOS-14, where TCANet can generate proposals with high precision and recall. By combining with the existing action classifier, TCANet can obtain remarkable temporal action detection performance compared with other methods. Not surprisingly, the proposed TCANet won the 1st place in the CVPR 2020 - HACS challenge leaderboard on temporal action localization task.},
keywords = {location awareness;computer vision;benchmark testing;reliability engineering;pattern recognition;proposals;task analysis},
doi = {10.1109/CVPR46437.2021.00055},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.00055},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@InProceedings{10.1007/978-3-319-46487-9_47,
author="Escorcia, Victor
and Caba Heilbron, Fabian
and Niebles, Juan Carlos
and Ghanem, Bernard",
editor="Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max",
title="DAPs: Deep Action Proposals for Action Understanding",
booktitle="Computer Vision -- ECCV 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="768--784",
abstract="Object proposals have contributed significantly to recent advances in object understanding in images. Inspired by the success of this approach, we introduce Deep Action Proposals (DAPs), an effective and efficient algorithm for generating temporal action proposals from long videos. We show how to take advantage of the vast capacity of deep learning models and memory cells to retrieve from untrimmed videos temporal segments, which are likely to contain actions. A comprehensive evaluation indicates that our approach outperforms previous work on a large scale action benchmark, runs at 134 FPS making it practical for large-scale scenarios, and exhibits an appealing ability to generalize, i.e. to retrieve good quality temporal proposals of actions unseen in training.",
isbn="978-3-319-46487-9"
}

@INPROCEEDINGS{10203543,
  author={Shi, Dingfeng and Zhong, Yujie and Cao, Qiong and Ma, Lin and Lit, Jia and Tao, Dacheng},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={TriDet: Temporal Action Detection with Relative Boundary Modeling}, 
  year={2023},
  volume={},
  number={},
  pages={18857-18866},
  keywords={Convolutional codes;Computer vision;Computational modeling;Aggregates;Benchmark testing;Probability distribution;Pattern recognition;Video: Action and event understanding},
  doi={10.1109/CVPR52729.2023.01808}}


@inproceedings{zhang2022actionformer,
  title={ActionFormer: Localizing Moments of Actions with Transformers},
  author={Zhang, Chen-Lin and Wu, Jianxin and Li, Yin},
  booktitle={European Conference on Computer Vision},
  series={LNCS},
  volume={13664},
  pages={492-510},
  year={2022}
}

@INPROCEEDINGS{tridet,
  author={Shi, Dingfeng and Zhong, Yujie and Cao, Qiong and Ma, Lin and Lit, Jia and Tao, Dacheng},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={TriDet: Temporal Action Detection with Relative Boundary Modeling}, 
  year={2023},
  volume={},
  number={},
  pages={18857-18866},
  keywords={Convolutional codes;Computer vision;Computational modeling;Aggregates;Benchmark testing;Probability distribution;Pattern recognition;Video: Action and event understanding},
  doi={10.1109/CVPR52729.2023.01808}}


@ARTICLE{9171561,
  author={Yang, Le and Peng, Houwen and Zhang, Dingwen and Fu, Jianlong and Han, Junwei},
  journal={IEEE Transactions on Image Processing}, 
  title={Revisiting Anchor Mechanisms for Temporal Action Localization}, 
  year={2020},
  volume={29},
  number={},
  pages={8535-8548},
  keywords={Pipelines;Proposals;Videos;Task analysis;Detectors;Object detection;Automation;Temporal action localization;default anchor;anchor free;complementarity},
  doi={10.1109/TIP.2020.3016486}}

@inproceedings{ActivityNet,
author = {Heilbron, Fabian and Escorcia, Victor and Ghanem, Bernard and Niebles, Juan Carlos},
year = {2015},
month = {06},
pages = {},
title = {ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding},
doi = {10.1109/CVPR.2015.7298698}
}

@misc{damen2018scalingegocentricvisionepickitchens,
      title={Scaling Egocentric Vision: The EPIC-KITCHENS Dataset}, 
      author={Dima Damen and Hazel Doughty and Giovanni Maria Farinella and Sanja Fidler and Antonino Furnari and Evangelos Kazakos and Davide Moltisanti and Jonathan Munro and Toby Perrett and Will Price and Michael Wray},
      year={2018},
      eprint={1804.02748},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1804.02748}, 
}

@article{Idrees_2017,
   title={The THUMOS challenge on action recognition for videos “in the wild”},
   volume={155},
   ISSN={1077-3142},
   url={http://dx.doi.org/10.1016/j.cviu.2016.10.018},
   DOI={10.1016/j.cviu.2016.10.018},
   journal={Computer Vision and Image Understanding},
   publisher={Elsevier BV},
   author={Idrees, Haroon and Zamir, Amir R. and Jiang, Yu-Gang and Gorban, Alex and Laptev, Ivan and Sukthankar, Rahul and Shah, Mubarak},
   year={2017},
   month=feb, pages={1–23} }

