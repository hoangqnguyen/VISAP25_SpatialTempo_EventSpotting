@misc{tdeed23,
  title         = {T-DEED: Temporal-Discriminability Enhancer Encoder-Decoder for Precise Event Spotting in Sports Videos},
  author        = {Artur Xarles and Sergio Escalera and Thomas B. Moeslund and Albert Clapés},
  year          = {2024},
  eprint        = {2404.05392},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2404.05392}
}
@misc{spot22,
  title         = {Spotting Temporally Precise, Fine-Grained Events in Video},
  author        = {James Hong and Haotian Zhang and Michaël Gharbi and Matthew Fisher and Kayvon Fatahalian},
  year          = {2022},
  eprint        = {2207.10213},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2207.10213}
}

@inproceedings{soccernetv2,
  title     = {Soccernet-v2: A dataset and benchmarks for holistic understanding of broadcast soccer videos},
  author    = {Deliege, Adrien and Cioppa, Anthony and Giancola, Silvio and Seikavandi, Meisam J and Dueholm, Jacob V and Nasrollahi, Kamal and Ghanem, Bernard and Moeslund, Thomas B and Van Droogenbroeck, Marc},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages     = {4508--4519},
  year      = {2021}
}

@inproceedings{soccernet,
  title     = {SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos},
  url       = {http://dx.doi.org/10.1109/CVPRW.2018.00223},
  doi       = {10.1109/cvprw.2018.00223},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  publisher = {IEEE},
  author    = {Giancola, Silvio and Amine, Mohieddine and Dghaily, Tarek and Ghanem, Bernard},
  year      = {2018},
  month     = jun
}


@misc{finediving,
  title         = {FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment},
  author        = {Jinglin Xu and Yongming Rao and Xumin Yu and Guangyi Chen and Jie Zhou and Jiwen Lu},
  year          = {2022},
  eprint        = {2204.03646},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2204.03646}
}

@inproceedings{figureskating,
  author    = {Hong, James and Fisher, Matthew and Gharbi, Michaël and Fatahalian, Kayvon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Video Pose Distillation for Few-Shot, Fine-Grained Sports Action Recognition},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {9234-9243},
  keywords  = {Visualization;Target recognition;Impedance matching;Pose estimation;Detectors;Feature extraction;Pattern recognition;Transfer/Low-shot/Semi/Unsupervised Learning;Action and behavior recognition;Gestures and body pose;Representation learning;Video analysis and understanding},
  doi       = {10.1109/ICCV48922.2021.00912}
}

@inproceedings{tsn,
  author    = {Wang, Limin
               and Xiong, Yuanjun
               and Wang, Zhe
               and Qiao, Yu
               and Lin, Dahua
               and Tang, Xiaoou
               and Van Gool, Luc},
  editor    = {Leibe, Bastian
               and Matas, Jiri
               and Sebe, Nicu
               and Welling, Max},
  title     = {Temporal Segment Networks: Towards Good Practices for Deep Action Recognition},
  booktitle = {Computer Vision -- ECCV 2016},
  year      = {2016},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {20--36},
  abstract  = {Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ({\$}{\$} 69.4{\backslash},{\backslash}{\%} {\$}{\$}) and UCF101 ({\$}{\$} 94.2{\backslash},{\backslash}{\%} {\$}{\$}). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices (Models and code at https://github.com/yjxiong/temporal-segment-networks).},
  isbn      = {978-3-319-46484-8}
}

@INPROCEEDINGS{8578773,
  author={Tran, Du and Wang, Heng and Torresani, Lorenzo and Ray, Jamie and LeCun, Yann and Paluri, Manohar},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={A Closer Look at Spatiotemporal Convolutions for Action Recognition}, 
  year={2018},
  volume={},
  number={},
  pages={6450-6459},
  keywords={Three-dimensional displays;Two dimensional displays;Spatiotemporal phenomena;Solid modeling;Feature extraction;Computer architecture},
  doi={10.1109/CVPR.2018.00675}}

@misc{zhou2018temporalrelationalreasoningvideos,
      title={Temporal Relational Reasoning in Videos}, 
      author={Bolei Zhou and Alex Andonian and Aude Oliva and Antonio Torralba},
      year={2018},
      eprint={1711.08496},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1711.08496}, 
}

@misc{radosavovic2020designingnetworkdesignspaces,
      title={Designing Network Design Spaces}, 
      author={Ilija Radosavovic and Raj Prateek Kosaraju and Ross Girshick and Kaiming He and Piotr Dollár},
      year={2020},
      eprint={2003.13678},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2003.13678}, 
}


@INPROCEEDINGS{9156729,
  author={Sudhakaran, Swathikiran and Escalera, Sergio and Lanz, Oswald},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Gate-Shift Networks for Video Action Recognition}, 
  year={2020},
  volume={},
  number={},
  pages={1099-1108},
  keywords={Two dimensional displays;Three-dimensional displays;GSM;Convolution;Feature extraction;Logic gates;Kernel},
  doi={10.1109/CVPR42600.2020.00118}}
















